import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib import rcParams
import os
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.layers import Input, Conv1D, LSTM, Dense, Dropout
from tensorflow.keras.layers import MaxPooling1D
from tensorflow.keras.layers import Attention
from tensorflow.keras.models import Model
import tensorflow as tf
import pickle


def save_to_csv(time_data, predicted_data_inversed, filename):
    # å¦‚æœæ˜¯äºŒç»´æ•°ç»„æˆ–å…¶ä»–ç±»å‹ï¼Œå¯ä»¥å±•å¹³ä¸ºä¸€ç»´æ•°ç»„
    time_data = time_data.flatten() if hasattr(time_data, 'flatten') else time_data
    predicted_data_inversed = predicted_data_inversed.flatten() if hasattr(predicted_data_inversed, 'flatten') else predicted_data_inversed

    # åˆ›å»º DataFrame
    df = pd.DataFrame({
        'time': time_data,
        'predicted': predicted_data_inversed
    })

    # ä¿å­˜ä¸º CSV æ–‡ä»¶
    df.to_csv(filename, index=False)


def save_comparison_plot(true_data, pred_data, time_true, time_pred,
                            filename='prediction_comparison.png', dpi=300):
    # åˆ›å»ºç”»å¸ƒï¼ˆé€‚é…å¤§é‡æ•°æ®çš„å°ºå¯¸ï¼‰
    fig, ax = plt.subplots(figsize=(15, 6), dpi=dpi)

    # ç»˜åˆ¶çœŸå®æ•°æ®ï¼ˆç»¿è‰²ï¼‰
    ax.plot(time_true, true_data['Voltage'].values,
            color='#2ecc71', label='True Data', alpha=0.8, linewidth=0.8)
    # ç»˜åˆ¶é¢„æµ‹æ•°æ®ï¼ˆçº¢è‰²ï¼‰
    ax.plot(time_pred, pred_data,
            color='#e74c3c', label='Predicted Data', alpha=0.8, linewidth=0.8)

    # å›¾è¡¨é…ç½®ï¼ˆè‹±æ–‡æ ‡ç­¾ï¼Œé¿å…å­—ä½“é—®é¢˜ï¼‰
    ax.set_xlabel('Sample Index', fontsize=12)
    ax.set_ylabel('Voltage (V)', fontsize=12)
    ax.set_title('True vs Predicted Voltage Comparison', fontsize=14, fontweight='bold')
    ax.legend(fontsize=10)
    ax.grid(alpha=0.3)

    # ç´§å‡‘å¸ƒå±€ï¼Œé¿å…è£å‰ª
    plt.tight_layout()
    # ä¿å­˜å›¾ç‰‡ï¼ˆé«˜æ¸…ï¼Œæ— ç™½è¾¹ï¼‰
    plt.savefig(filename, bbox_inches='tight', pad_inches=0.1)
    plt.close()  # å…³é—­ç”»å¸ƒï¼Œé‡Šæ”¾å†…å­˜

    print(f"âœ… å¯¹æ¯”å›¾å·²ä¿å­˜ä¸ºPNGï¼š{os.path.abspath(filename)}")
    return os.path.abspath(filename)

def save_prediction_to_csv(pred_data, filename='prediction_result.csv'):
    # å±•å¹³æ•°æ®ä¸ºä¸€ç»´ï¼Œç¡®ä¿å•åˆ—
    pred_data_flat = pred_data.flatten()
    # åˆ›å»ºå•åˆ—DataFrame
    df = pd.DataFrame({
        'Predicted_Voltage': pred_data_flat
    })
    # ä¿å­˜CSVï¼ˆæ— ç´¢å¼•ï¼Œä»…æ•°æ®ï¼‰
    df.to_csv(filename, index=False, header=False)
    print(f"âœ… é¢„æµ‹æ•°æ®å·²ä¿å­˜ä¸ºCSVï¼š{os.path.abspath(filename)}")
    return os.path.abspath(filename)

def ensemble_loss(y_true, y_pred, a=0.3, b=0.7):
    # è‡ªå®šä¹‰é›†æˆæŸå¤±å‡½æ•°
    loss1 = tf.square(y_true - y_pred)
    loss2 = tf.abs(y_true - y_pred)
    return a * loss1 + b * loss2


# å…¨å±€æ ‡å‡†åŒ–å‡½æ•°ï¼ˆè®­ç»ƒå’Œé¢„æµ‹ç”¨åŒä¸€ä¸ªscalerï¼‰
def create_scaler(data):
    """åŸºäºè®­ç»ƒæ•°æ®åˆ›å»ºscalerï¼Œä¾›åç»­é¢„æµ‹ä½¿ç”¨"""
    scaler = MinMaxScaler()
    scaler.fit(data[['Voltage']])
    return scaler


# æ„å»ºæ•°æ®é›†ï¼ˆä¸å˜ï¼‰
def create_sequences(data_scaled, seq_length=32):
    X, y = [], []
    for i in range(len(data_scaled) - seq_length):
        X.append(data_scaled[i:i + seq_length, 0])
        y.append(data_scaled[i + seq_length, 0])

    if len(X) == 0:
        raise ValueError(f"æ•°æ®é•¿åº¦ä¸è¶³ï¼è‡³å°‘éœ€è¦ {seq_length + 1} ä¸ªæ ·æœ¬ï¼ˆå½“å‰ï¼š{len(data_scaled)}ï¼‰")

    X = np.array(X)
    y = np.array(y)
    X = np.reshape(X, (X.shape[0], X.shape[1], 1))
    return X, y


# ç»Ÿä¸€æ¨¡å‹æ„å»ºå‡½æ•°ï¼ˆè®­ç»ƒ/é¢„æµ‹å…±ç”¨ï¼Œé¿å…ç»“æ„ä¸ä¸€è‡´ï¼‰
def build_unified_model(seq_length=32):
    """ç»Ÿä¸€çš„æ¨¡å‹ç»“æ„ï¼Œè®­ç»ƒå’Œé¢„æµ‹éƒ½ç”¨è¿™ä¸ª"""
    inputs = Input(shape=(seq_length, 1))
    conv1 = Conv1D(filters=64, kernel_size=3, activation='relu')(inputs)
    conv2 = Conv1D(filters=128, kernel_size=3, activation='relu')(conv1)
    pool1 = MaxPooling1D(pool_size=2)(conv2)
    lstm1 = LSTM(units=50, return_sequences=True)(pool1)
    attention_out = Attention()([lstm1, lstm1])
    lstm2 = LSTM(units=50, return_sequences=False)(attention_out)
    lstm2 = Dropout(0.1)(lstm2)
    outputs = Dense(1)(lstm2)
    model = Model(inputs=inputs, outputs=outputs)
    model.compile(optimizer='adam', loss=ensemble_loss)
    return model


# ä¸å¤ªè¡Œï¼Œå·²ç»å¼ƒä¹‹ä¸ç”¨ã€‚
def train_lstm_attention_model(preprocessed_df, seq_length=32, save_path='./',
                               roll_window_ratio=0.2, roll_step_ratio=0.1):
    """
    æ ¸å¿ƒæ”¹è¿›ï¼šä¼˜å…ˆåŠ è½½å·²æœ‰æ¨¡å‹/Scalerï¼Œæ— æ–‡ä»¶æ—¶æ‰é‡æ–°è®­ç»ƒ
    :return: æ¨¡å‹ã€scaler
    """
    # æ¨¡å‹/Scalerè·¯å¾„
    model_weights_path = os.path.join(save_path, 'lstm_model_weights.h5')
    scaler_path = os.path.join(save_path, 'scaler_piezo.pkl')

    # ========== å…³é”®ï¼šä¼˜å…ˆåŠ è½½å·²æœ‰æ–‡ä»¶ ==========
    if os.path.exists(model_weights_path) and os.path.exists(scaler_path):
        # åŠ è½½å·²æœ‰Scaler
        with open(scaler_path, 'rb') as f:
            scaler = pickle.load(f)
        # åŠ è½½å·²æœ‰æ¨¡å‹
        model = build_unified_model(seq_length)
        model.load_weights(model_weights_path)
        print(f"âœ… å·²åŠ è½½å·²æœ‰æ¨¡å‹ï¼š{model_weights_path}")
        print(f"âœ… å·²åŠ è½½å·²æœ‰Scalerï¼š{scaler_path}")
        return model, scaler

    # ========== æ— æ–‡ä»¶æ—¶æ‰é‡æ–°è®­ç»ƒ ==========
    print("âš ï¸ æœªæ£€æµ‹åˆ°å·²æœ‰æ¨¡å‹/Scalerï¼Œå¼€å§‹é‡æ–°è®­ç»ƒ...")
    scaler = create_scaler(preprocessed_df)
    data_scaled = scaler.transform(preprocessed_df[['Voltage']])

    # æ„å»ºå®Œæ•´åºåˆ—æ•°æ®é›†
    X_all, y_all = create_sequences(data_scaled, seq_length)
    total_samples = len(X_all)

    # æ»šåŠ¨éªŒè¯å‚æ•°è®¡ç®—
    val_window_size = int(total_samples * roll_window_ratio)
    roll_step = int(total_samples * roll_step_ratio)
    if val_window_size == 0 or roll_step == 0:
        raise ValueError("æ•°æ®é‡è¿‡å°ï¼Œæ— æ³•è¿›è¡Œæ»šåŠ¨éªŒè¯ï¼è¯·å¢å¤§æ•°æ®é‡æˆ–è°ƒæ•´çª—å£/æ­¥é•¿æ¯”ä¾‹")

    # åˆå§‹åŒ–æ¨¡å‹
    model = build_unified_model(seq_length)

    # æ»šåŠ¨éªŒè¯è®­ç»ƒ
    start_idx = 0
    val_loss_list = []
    while start_idx + val_window_size <= total_samples:
        # åˆ’åˆ†è®­ç»ƒ/éªŒè¯çª—å£
        train_end_idx = start_idx
        val_start_idx = train_end_idx
        val_end_idx = val_start_idx + val_window_size

        X_train = X_all[:train_end_idx] if train_end_idx > 0 else X_all[:val_start_idx]
        y_train = y_all[:train_end_idx] if train_end_idx > 0 else y_all[:val_start_idx]
        X_val = X_all[val_start_idx:val_end_idx]
        y_val = y_all[val_start_idx:val_end_idx]

        # è·³è¿‡æ ·æœ¬ä¸è¶³çš„æƒ…å†µ
        if len(X_train) < 100 or len(X_val) < 50:
            start_idx += roll_step
            continue

        # è®­ç»ƒå½“å‰çª—å£
        print(f"\n=== æ»šåŠ¨çª—å£ {start_idx // roll_step + 1} ===")
        print(f"è®­ç»ƒé›†ï¼š0 ~ {train_end_idx if train_end_idx > 0 else val_start_idx} æ ·æœ¬")
        print(f"éªŒè¯é›†ï¼š{val_start_idx} ~ {val_end_idx} æ ·æœ¬")

        history = model.fit(
            X_train, y_train,
            epochs=10,
            batch_size=32,
            validation_data=(X_val, y_val),
            verbose=1,
            shuffle=False
        )

        # è®°å½•éªŒè¯æŸå¤±
        val_loss = history.history['val_loss'][-1]
        val_loss_list.append(val_loss)
        print(f"å½“å‰çª—å£éªŒè¯æŸå¤±ï¼š{val_loss:.4f}")

        # æ»šåŠ¨åˆ°ä¸‹ä¸€ä¸ªçª—å£
        start_idx += roll_step

    # ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹å’Œscaler
    model.save_weights(model_weights_path)
    print(f"\nâœ… æ¨¡å‹æƒé‡å·²ä¿å­˜ï¼š{model_weights_path}")
    print(f"å¹³å‡éªŒè¯æŸå¤±ï¼š{np.mean(val_loss_list):.4f}")

    with open(scaler_path, 'wb') as f:
        pickle.dump(scaler, f)
    print(f"âœ… Scalerå·²ä¿å­˜ï¼š{scaler_path}")

    return model, scaler


# æ ¸å¿ƒï¼šé‡æ„åçš„æ»‘åŠ¨çª—å£è®­ç»ƒå‡½æ•°
def train_lstm_attention_model_2(preprocessed_df, seq_length=32, save_path='./',
                               window_ratio=0.1, val_ratio=0.2, step_ratio=0.5):
    """
    é€‚é…æ—¶åºç‰¹å¾å˜åŒ–çš„æ»‘åŠ¨çª—å£è®­ç»ƒå‡½æ•°
    :param preprocessed_df: é¢„å¤„ç†åçš„DataFrameï¼ˆå«Voltageåˆ—ï¼‰
    :param seq_length: åºåˆ—çª—å£é•¿åº¦
    :param save_path: æ¨¡å‹ä¿å­˜è·¯å¾„
    :param window_ratio: è®­ç»ƒçª—å£å æ€»æ ·æœ¬çš„æ¯”ä¾‹ï¼ˆé»˜è®¤10%ï¼‰
    :param val_ratio: éªŒè¯é›†å è®­ç»ƒçª—å£çš„æ¯”ä¾‹ï¼ˆé»˜è®¤20%ï¼‰
    :param step_ratio: çª—å£æ»‘åŠ¨æ­¥é•¿å è®­ç»ƒçª—å£çš„æ¯”ä¾‹ï¼ˆé»˜è®¤50%ï¼‰
    :return: æœ€ä¼˜æ¨¡å‹ã€å…¨å±€scaler
    """
    # æ¨¡å‹/Scalerè·¯å¾„ï¼ˆä¿ç•™åŸæœ‰å‘½åï¼‰
    model_weights_path = os.path.join(save_path, 'lstm_model_weights.h5')
    scaler_path = os.path.join(save_path, 'scaler_piezo.pkl')

    # ========== ä¼˜å…ˆåŠ è½½å·²æœ‰æ¨¡å‹ï¼ˆä¿ç•™åŸæœ‰é€»è¾‘ï¼‰ ==========
    if os.path.exists(model_weights_path) and os.path.exists(scaler_path):
        with open(scaler_path, 'rb') as f:
            scaler = pickle.load(f)
        model = build_unified_model(seq_length)
        model.load_weights(model_weights_path)
        print(f"âœ… å·²åŠ è½½å·²æœ‰æ¨¡å‹ï¼š{model_weights_path}")
        print(f"âœ… å·²åŠ è½½å·²æœ‰Scalerï¼š{scaler_path}")
        return model, scaler

    # ========== æ»‘åŠ¨çª—å£è®­ç»ƒæ ¸å¿ƒé€»è¾‘ ==========
    print("âš ï¸ æœªæ£€æµ‹åˆ°å·²æœ‰æ¨¡å‹/Scalerï¼Œå¼€å§‹æ»‘åŠ¨çª—å£è®­ç»ƒ...")

    # å…¨å±€æ ‡å‡†åŒ–ï¼ˆä¿ç•™åŸæœ‰é€»è¾‘ï¼Œå¦‚éœ€å±€éƒ¨æ ‡å‡†åŒ–å¯åœ¨æ­¤ä¿®æ”¹ï¼‰
    scaler = MinMaxScaler()
    data_scaled = scaler.fit_transform(preprocessed_df[['Voltage']])
    X_all, y_all = create_sequences(data_scaled, seq_length)
    total_samples = len(X_all)

    # è®¡ç®—æ»‘åŠ¨çª—å£å‚æ•°ï¼ˆé€‚é…æ—¶åºç‰¹å¾å˜åŒ–ï¼‰
    window_size = int(total_samples * window_ratio)  # è®­ç»ƒçª—å£å¤§å°
    val_size = int(window_size * val_ratio)  # éªŒè¯é›†å¤§å°
    step = int(window_size * step_ratio)  # çª—å£æ»‘åŠ¨æ­¥é•¿

    # è¾¹ç•Œæ ¡éªŒ
    if window_size < 100 or val_size < 20:
        raise ValueError("çª—å£è¿‡å°ï¼è¯·å¢å¤§window_ratioæˆ–ç¡®ä¿æ•°æ®é‡å……è¶³")
    if window_size + val_size >= total_samples:
        raise ValueError("çª—å£+éªŒè¯é›†è¶…è¿‡æ€»æ ·æœ¬ï¼è¯·å‡å°window_ratio/val_ratio")

    best_val_loss = float('inf')
    best_model_weights = None

    # æ»‘åŠ¨çª—å£è®­ç»ƒå¾ªç¯
    for start in range(0, total_samples - window_size - val_size, step):
        # 1. åˆ’åˆ†å½“å‰çª—å£çš„è®­ç»ƒ/éªŒè¯é›†ï¼ˆçº¯å±€éƒ¨æ•°æ®ï¼Œéç´¯åŠ ï¼‰
        X_train = X_all[start:start + window_size]
        y_train = y_all[start:start + window_size]
        X_val = X_all[start + window_size:start + window_size + val_size]
        y_val = y_all[start + window_size:start + window_size + val_size]

        # 2. æ¯ä¸ªçª—å£é‡æ–°åˆå§‹åŒ–æ¨¡å‹ï¼ˆé¿å…å¢é‡è¿‡æ‹Ÿåˆï¼‰
        model = build_unified_model(seq_length)

        # 3. å°‘é‡epochè®­ç»ƒï¼ˆæ—¶åºçª—å£é¿å…è¿‡æ‹Ÿåˆï¼‰
        print(f"\n=== æ»‘åŠ¨çª—å£ {start // step + 1} ===")
        print(f"è®­ç»ƒçª—å£ï¼š{start} ~ {start + window_size}ï¼ˆå…±{len(X_train)}æ ·æœ¬ï¼‰")
        print(f"éªŒè¯çª—å£ï¼š{start + window_size} ~ {start + window_size + val_size}ï¼ˆå…±{len(X_val)}æ ·æœ¬ï¼‰")

        history = model.fit(
            X_train, y_train,
            epochs=3,  # æ—¶åºçª—å£è®­ç»ƒepochä¸å®œè¿‡å¤š
            batch_size=32,
            validation_data=(X_val, y_val),
            verbose=1,
            shuffle=False  # æ—¶åºæ•°æ®ç¦æ­¢shuffle
        )

        # 4. è®°å½•æœ€ä¼˜æ¨¡å‹ï¼ˆä¿ç•™éªŒè¯æŸå¤±æœ€ä½çš„ï¼‰
        current_val_loss = history.history['val_loss'][-1]
        print(f"å½“å‰çª—å£éªŒè¯æŸå¤±ï¼š{current_val_loss:.4f}")

        if current_val_loss < best_val_loss:
            best_val_loss = current_val_loss
            best_model_weights = model.get_weights()
            print(f"ğŸ“ˆ æ›´æ–°æœ€ä¼˜æ¨¡å‹ï¼ˆéªŒè¯æŸå¤±ï¼š{best_val_loss:.4f}ï¼‰")

    # ========== ä¿å­˜æœ€ä¼˜æ¨¡å‹ ==========
    if best_model_weights is None:
        raise RuntimeError("æ— æœ‰æ•ˆè®­ç»ƒçª—å£ï¼è¯·æ£€æŸ¥æ•°æ®é‡æˆ–çª—å£å‚æ•°")

    # åŠ è½½æœ€ä¼˜æƒé‡å¹¶ä¿å­˜
    final_model = build_unified_model(seq_length)
    final_model.set_weights(best_model_weights)
    final_model.save_weights(model_weights_path)

    # ä¿å­˜scaler
    with open(scaler_path, 'wb') as f:
        pickle.dump(scaler, f)

    print(f"\nâœ… æ»‘åŠ¨çª—å£è®­ç»ƒå®Œæˆï¼")
    print(f"ğŸ“Š æœ€ä¼˜éªŒè¯æŸå¤±ï¼š{best_val_loss:.4f}")
    print(f"ğŸ’¾ æ¨¡å‹ä¿å­˜è‡³ï¼š{model_weights_path}")
    print(f"ğŸ’¾ Scalerä¿å­˜è‡³ï¼š{scaler_path}")

    return final_model, scaler


def train_lstm_attention_model_local_scaler(preprocessed_df, seq_length=32, save_path='./',
                                            window_ratio=0.1, val_ratio=0.2, step_ratio=0.5):
    """
    é€‚é…æ—¶åºç‰¹å¾å˜åŒ–çš„æ»‘åŠ¨çª—å£è®­ç»ƒå‡½æ•°ï¼ˆå±€éƒ¨çª—å£æ ‡å‡†åŒ–ï¼‰
    :param preprocessed_df: é¢„å¤„ç†åçš„DataFrameï¼ˆå«Voltageåˆ—ï¼‰
    :param seq_length: åºåˆ—çª—å£é•¿åº¦
    :param save_path: æ¨¡å‹ä¿å­˜è·¯å¾„
    :param window_ratio: è®­ç»ƒçª—å£å æ€»æ ·æœ¬çš„æ¯”ä¾‹ï¼ˆé»˜è®¤10%ï¼‰
    :param val_ratio: éªŒè¯é›†å è®­ç»ƒçª—å£çš„æ¯”ä¾‹ï¼ˆé»˜è®¤20%ï¼‰
    :param step_ratio: çª—å£æ»‘åŠ¨æ­¥é•¿å è®­ç»ƒçª—å£çš„æ¯”ä¾‹ï¼ˆé»˜è®¤50%ï¼‰
    :return: æœ€ä¼˜æ¨¡å‹ã€å„çª—å£scalerå­—å…¸ã€å…¨å±€å‚è€ƒscalerï¼ˆç”¨äºå…œåº•ï¼‰
    """
    # æ¨¡å‹/Scalerè·¯å¾„ï¼ˆæ–°å¢çª—å£scalerä¿å­˜ï¼‰
    model_weights_path = os.path.join(save_path, 'lstm_model_weights_local.h5')
    window_scalers_path = os.path.join(save_path, 'window_scalers.pkl')
    global_scaler_path = os.path.join(save_path, 'global_scaler_piezo.pkl')

    # ========== ä¼˜å…ˆåŠ è½½å·²æœ‰æ¨¡å‹ï¼ˆå…¼å®¹å±€éƒ¨scalerï¼‰ ==========
    if os.path.exists(model_weights_path) and os.path.exists(window_scalers_path) and os.path.exists(
            global_scaler_path):
        with open(window_scalers_path, 'rb') as f:
            window_scalers = pickle.load(f)
        with open(global_scaler_path, 'rb') as f:
            global_scaler = pickle.load(f)
        model = build_unified_model(seq_length)
        model.load_weights(model_weights_path)
        print(f"âœ… å·²åŠ è½½å·²æœ‰æ¨¡å‹ï¼š{model_weights_path}")
        print(f"âœ… å·²åŠ è½½çª—å£scalerï¼š{window_scalers_path}")
        print(f"âœ… å·²åŠ è½½å…¨å±€å‚è€ƒscalerï¼š{global_scaler_path}")
        return model, window_scalers, global_scaler

    # ========== æ»‘åŠ¨çª—å£è®­ç»ƒæ ¸å¿ƒé€»è¾‘ï¼ˆå±€éƒ¨æ ‡å‡†åŒ–ï¼‰ ==========
    print("âš ï¸ æœªæ£€æµ‹åˆ°å·²æœ‰æ¨¡å‹/Scalerï¼Œå¼€å§‹å±€éƒ¨æ ‡å‡†åŒ–æ»‘åŠ¨çª—å£è®­ç»ƒ...")

    # å…ˆæ„å»ºå…¨å±€åºåˆ—ï¼ˆç”¨åŸå§‹æ•°æ®ï¼Œä¸åšå…¨å±€æ ‡å‡†åŒ–ï¼‰
    raw_data = preprocessed_df[['Voltage']].values
    total_samples_raw = len(raw_data) - seq_length
    if total_samples_raw < 100:
        raise ValueError("åŸå§‹æ•°æ®é‡ä¸è¶³ï¼è‡³å°‘éœ€è¦100ä¸ªæœ‰æ•ˆæ ·æœ¬")

    # è®¡ç®—æ»‘åŠ¨çª—å£å‚æ•°ï¼ˆåŸºäºåŸå§‹æ•°æ®ï¼‰
    window_size = int(total_samples_raw * window_ratio)  # è®­ç»ƒçª—å£å¤§å°
    val_size = int(window_size * val_ratio)  # éªŒè¯é›†å¤§å°
    step = int(window_size * step_ratio)  # çª—å£æ»‘åŠ¨æ­¥é•¿

    # è¾¹ç•Œæ ¡éªŒ
    if window_size < 100 or val_size < 20:
        raise ValueError("çª—å£è¿‡å°ï¼è¯·å¢å¤§window_ratioæˆ–ç¡®ä¿æ•°æ®é‡å……è¶³")
    if window_size + val_size >= total_samples_raw:
        raise ValueError("çª—å£+éªŒè¯é›†è¶…è¿‡æ€»æ ·æœ¬ï¼è¯·å‡å°window_ratio/val_ratio")

    best_val_loss = float('inf')
    best_model_weights = None
    window_scalers = {}  # ä¿å­˜æ¯ä¸ªçª—å£çš„scalerï¼š{çª—å£èµ·å§‹ä½ç½®: scalerå¯¹è±¡}

    # æ»‘åŠ¨çª—å£è®­ç»ƒå¾ªç¯
    for start in range(0, total_samples_raw - window_size - val_size, step):
        # 1. å–å½“å‰çª—å£çš„åŸå§‹æ•°æ®ï¼ˆéæ ‡å‡†åŒ–ï¼‰
        # åŸå§‹æ•°æ®çš„ç´¢å¼•ï¼šåºåˆ—èµ·å§‹ä½ç½® = start â†’ åºåˆ—ç»“æŸä½ç½® = start + window_size + val_size + seq_length
        raw_start = start
        raw_end_train = start + window_size + seq_length
        raw_end_val = raw_end_train + val_size

        # è®­ç»ƒçª—å£åŸå§‹æ•°æ®
        raw_train_data = raw_data[raw_start:raw_end_train]
        # éªŒè¯çª—å£åŸå§‹æ•°æ®
        raw_val_data = raw_data[raw_end_train - seq_length:raw_end_val]

        # 2. å±€éƒ¨æ ‡å‡†åŒ–ï¼šæ¯ä¸ªçª—å£å•ç‹¬è®­ç»ƒscaler
        local_scaler = MinMaxScaler()
        train_scaled = local_scaler.fit_transform(raw_train_data)
        val_scaled = local_scaler.transform(raw_val_data)

        # 3. æ„å»ºå½“å‰çª—å£çš„åºåˆ—
        X_train, y_train = create_sequences(train_scaled, seq_length)
        X_val, y_val = create_sequences(val_scaled, seq_length)

        # è·³è¿‡æ ·æœ¬ä¸è¶³çš„æƒ…å†µ
        if len(X_train) < 50 or len(X_val) < 10:
            print(f"âš ï¸ çª—å£{start // step + 1}æ ·æœ¬ä¸è¶³ï¼Œè·³è¿‡")
            continue

        # 4. æ¯ä¸ªçª—å£é‡æ–°åˆå§‹åŒ–æ¨¡å‹ï¼ˆé¿å…å¢é‡è¿‡æ‹Ÿåˆï¼‰
        model = build_unified_model(seq_length)

        # 5. å°‘é‡epochè®­ç»ƒï¼ˆæ—¶åºçª—å£é¿å…è¿‡æ‹Ÿåˆï¼‰
        print(f"\n=== æ»‘åŠ¨çª—å£ {start // step + 1} ===")
        print(f"è®­ç»ƒçª—å£åŸå§‹æ•°æ®ï¼š{raw_start} ~ {raw_end_train}ï¼ˆå…±{len(raw_train_data)}æ ·æœ¬ï¼‰")
        print(f"éªŒè¯çª—å£åŸå§‹æ•°æ®ï¼š{raw_end_train - seq_length} ~ {raw_end_val}ï¼ˆå…±{len(raw_val_data)}æ ·æœ¬ï¼‰")
        print(f"è®­ç»ƒåºåˆ—æ•°ï¼š{len(X_train)} | éªŒè¯åºåˆ—æ•°ï¼š{len(X_val)}")

        history = model.fit(
            X_train, y_train,
            epochs=3,  # æ—¶åºçª—å£è®­ç»ƒepochä¸å®œè¿‡å¤š
            batch_size=32,
            validation_data=(X_val, y_val),
            verbose=1,
            shuffle=False  # æ—¶åºæ•°æ®ç¦æ­¢shuffle
        )

        # 6. è®°å½•å½“å‰çª—å£scalerå’Œæœ€ä¼˜æ¨¡å‹
        window_scalers[start] = local_scaler
        current_val_loss = history.history['val_loss'][-1]
        print(f"å½“å‰çª—å£éªŒè¯æŸå¤±ï¼š{current_val_loss:.4f}")

        if current_val_loss < best_val_loss:
            best_val_loss = current_val_loss
            best_model_weights = model.get_weights()
            print(f"ğŸ“ˆ æ›´æ–°æœ€ä¼˜æ¨¡å‹ï¼ˆéªŒè¯æŸå¤±ï¼š{best_val_loss:.4f}ï¼‰")

    # ========== ä¿å­˜æœ€ä¼˜æ¨¡å‹å’Œscaler ==========
    if best_model_weights is None:
        raise RuntimeError("æ— æœ‰æ•ˆè®­ç»ƒçª—å£ï¼è¯·æ£€æŸ¥æ•°æ®é‡æˆ–çª—å£å‚æ•°")

    # åŠ è½½æœ€ä¼˜æƒé‡å¹¶ä¿å­˜
    final_model = build_unified_model(seq_length)
    final_model.set_weights(best_model_weights)
    final_model.save_weights(model_weights_path)

    # ä¿å­˜çª—å£scalerå’Œå…¨å±€å‚è€ƒscalerï¼ˆå…¨å±€scalerç”¨äºé¢„æµ‹æ—¶å…œåº•ï¼‰
    global_scaler = MinMaxScaler()
    global_scaler.fit(raw_data)
    with open(window_scalers_path, 'wb') as f:
        pickle.dump(window_scalers, f)
    with open(global_scaler_path, 'wb') as f:
        pickle.dump(global_scaler, f)

    print(f"\nâœ… å±€éƒ¨æ ‡å‡†åŒ–æ»‘åŠ¨çª—å£è®­ç»ƒå®Œæˆï¼")
    print(f"ğŸ“Š æœ€ä¼˜éªŒè¯æŸå¤±ï¼š{best_val_loss:.4f}")
    print(f"ğŸ’¾ æ¨¡å‹ä¿å­˜è‡³ï¼š{model_weights_path}")
    print(f"ğŸ’¾ çª—å£scalerä¿å­˜è‡³ï¼š{window_scalers_path}")
    print(f"ğŸ’¾ å…¨å±€å‚è€ƒscalerä¿å­˜è‡³ï¼š{global_scaler_path}")
    print(f"ğŸ“‹ å…±è®­ç»ƒ{len(window_scalers)}ä¸ªæœ‰æ•ˆçª—å£")

    return final_model, window_scalers, global_scaler

def predict_with_sliding_window_fixed(dataB, seq_length=32, model_weights_path='./lstm_model_weights.h5',
                                      scaler_path='./scaler_piezo.pkl', future_steps=16, target_total_points=None):
    """
    ä¿ç•™çš„é¢„æµ‹å‡½æ•°ï¼ˆä¿®å¤é‡å¤åŠ è½½æ¨¡å‹é—®é¢˜ï¼‰
    """
    # åŠ è½½Scaler
    with open(scaler_path, 'rb') as f:
        scaler = pickle.load(f)

    # åŠ è½½æ¨¡å‹ï¼ˆä»…ç”¨ç»Ÿä¸€ç»“æ„+æƒé‡åŠ è½½ï¼Œä¸é‡å¤load_modelï¼‰
    model = build_unified_model(seq_length)
    model.load_weights(model_weights_path)
    print(f"âœ… æˆåŠŸåŠ è½½æ¨¡å‹æƒé‡ï¼š{model_weights_path}")

    # æ•°æ®æ ‡å‡†åŒ–
    col_name = 'Voltage' if 'Voltage' in dataB.columns else 'CH1V'
    dataB_scaled = scaler.transform(dataB[[col_name]])

    # æ„å»ºæµ‹è¯•é›†çª—å£
    X_test = []
    for i in range(len(dataB_scaled) - seq_length):
        X_test.append(dataB_scaled[i:i + seq_length, 0])
    X_test = np.array(X_test).reshape(-1, seq_length, 1)

    # è®¡ç®—é¢„æµ‹æ®µæ•°
    max_possible_steps = len(X_test)
    if target_total_points is not None:
        total_steps = int(np.ceil(target_total_points / future_steps))
    else:
        total_steps = max_possible_steps
    total_steps = min(total_steps, max_possible_steps)
    if total_steps == 0:
        raise ValueError(f"æ•°æ®Bé•¿åº¦ä¸è¶³ï¼X_testä»…{len(X_test)}ä¸ªæ ·æœ¬")

    # é¢„æµ‹é€»è¾‘
    all_predicted_data = []
    time_list = []
    for step in range(total_steps):
        start_index = step * future_steps
        if start_index >= len(X_test):
            break
        last_sequence = X_test[start_index].reshape(1, seq_length, 1)

        predicted_data = []
        for i in range(future_steps):
            predicted_value = model.predict(last_sequence, verbose=0)
            predicted_data.append(predicted_value[0, 0])
            last_sequence = np.roll(last_sequence, -1, axis=1)
            last_sequence[0, -1, 0] = predicted_value[0, 0]

        all_predicted_data.extend(predicted_data)
        start_point = start_index + seq_length
        time_list.extend([start_point + i for i in range(len(predicted_data))])

    # é€†æ ‡å‡†åŒ–
    all_predicted_data = np.array(all_predicted_data).reshape(-1, 1)
    all_predicted_data_inversed = scaler.inverse_transform(all_predicted_data)
    time_data = np.array(time_list)

    print(f"âœ… é¢„æµ‹å®Œæˆï¼šå…±åˆ†{total_steps}æ®µï¼Œå•æ¬¡é¢„æµ‹{future_steps}ç‚¹ï¼Œæ€»é¢„æµ‹{len(all_predicted_data)}ç‚¹")
    return time_data, all_predicted_data_inversed


def predict_stepped_window_fast(dataB, seq_length=32, model_weights_path='./lstm_model_weights.h5',
                                scaler_path='./scaler_piezo.pkl', predict_step=32, target_total_points=None):
    """
    ä½ å®é™…è°ƒç”¨çš„é¢„æµ‹å‡½æ•°ï¼š
    1. å¤ç”¨ç»Ÿä¸€æ¨¡å‹ç»“æ„
    2. å…³é—­è‡ªå›å½’ï¼Œç”¨çœŸå®çª—å£æ»šåŠ¨
    3. ä¿ç•™å…³é”®è°ƒè¯•æ‰“å°
    """
    # 1. åŠ è½½Scalerå¹¶éªŒè¯èŒƒå›´
    with open(scaler_path, 'rb') as f:
        scaler = pickle.load(f)
    print(f"ğŸ” ScalerèŒƒå›´éªŒè¯ï¼šmin={scaler.data_min_[0]}, max={scaler.data_max_[0]}")

    # 2. åŠ è½½æ¨¡å‹ï¼ˆç”¨ç»Ÿä¸€ç»“æ„ï¼Œç¡®ä¿å’Œè®­ç»ƒä¸€è‡´ï¼‰
    model = build_unified_model(seq_length)
    model.load_weights(model_weights_path)
    tf.config.experimental.sync_to_device = False
    print(f"âœ… æˆåŠŸåŠ è½½æ¨¡å‹æƒé‡ï¼š{model_weights_path}")

    # 3. æ•°æ®æ ‡å‡†åŒ–
    col_name = 'Voltage' if 'Voltage' in dataB.columns else 'CH1V'
    data_scaled = scaler.transform(dataB[[col_name]])
    data_length = len(data_scaled)
    print(f"ğŸ” è¾“å…¥æ•°æ®æ ‡å‡†åŒ–åèŒƒå›´ï¼šmin={data_scaled.min()}, max={data_scaled.max()}")

    # 4. ç¡®å®šç›®æ ‡é¢„æµ‹é‡
    if target_total_points is None:
        target_total_points = data_length
    target_total_points = min(target_total_points, data_length - seq_length)

    # 5. æ ¸å¿ƒé¢„æµ‹é€»è¾‘ï¼ˆçœŸå®çª—å£æ»šåŠ¨ï¼Œä¸è‡ªå›å½’ï¼‰
    all_pred_time = []
    all_pred_data = []
    current_start = 0

    while len(all_pred_data) < target_total_points:
        if current_start + seq_length + predict_step > data_length:
            break

        # å–çœŸå®æ•°æ®çª—å£
        current_window = data_scaled[current_start:current_start + seq_length, 0]
        current_window = current_window.reshape(1, seq_length, 1)

        # å•æ¬¡é¢„æµ‹predict_stepä¸ªç‚¹ï¼ˆä»…ç”¨çœŸå®çª—å£ï¼‰
        predicted_data = []
        for i in range(predict_step):
            pred = model.predict(current_window, verbose=0)[0, 0]
            predicted_data.append(pred)

        # è®¡ç®—é¢„æµ‹ç‚¹åºå·
        pred_start = current_start + seq_length
        pred_time = [pred_start + i for i in range(predict_step)]

        # ç´¯åŠ ç»“æœ
        all_pred_data.extend(predicted_data)
        all_pred_time.extend(pred_time)

        # æ­¥è¿›
        current_start += predict_step

        progress = min(len(all_pred_data) / target_total_points * 100, 100)
        print(f"é¢„æµ‹è¿›åº¦ï¼š{progress:.1f}% | å·²é¢„æµ‹ï¼š{len(all_pred_data)}/{target_total_points} ç‚¹", end='\r')

    # 6. æˆªæ–­+é€†æ ‡å‡†åŒ–
    all_pred_data = all_pred_data[:target_total_points]
    all_pred_time = all_pred_time[:target_total_points]
    print(f"\nğŸ” æ¨¡å‹é¢„æµ‹å€¼èŒƒå›´ï¼ˆæ ‡å‡†åŒ–åï¼‰ï¼šmin={np.min(all_pred_data)}, max={np.max(all_pred_data)}")

    pred_data_scaled = np.array(all_pred_data).reshape(-1, 1)
    pred_data_inversed = scaler.inverse_transform(pred_data_scaled)
    print(f"ğŸ” é€†æ ‡å‡†åŒ–åé¢„æµ‹å€¼èŒƒå›´ï¼šmin={pred_data_inversed.min()}, max={pred_data_inversed.max()}")

    pred_time = np.array(all_pred_time)

    print(f"\nâœ… æ­¥è¿›å¼é¢„æµ‹å®Œæˆï¼šæ€»é¢„æµ‹{len(pred_data_inversed)}ç‚¹ï¼Œå•æ¬¡çª—å£{seq_length}ï¼Œå•æ¬¡é¢„æµ‹{predict_step}ç‚¹")
    return pred_time, pred_data_inversed


# æ ¸å¿ƒï¼šå¤åˆ»æ—§ä»£ç çš„é¢„æµ‹é€»è¾‘ï¼ˆè‡ªå›å½’+å°æ­¥é¢„æµ‹ï¼‰
def predict_old(dataB, seq_length=32, save_path='./', ratio=2):
    """
    é¢„æµ‹å‡½æ•°ï¼šå®Œå…¨å¯¹é½æ—§ä»£ç çš„æ ¸å¿ƒæ€è·¯ï¼ˆè‡ªå›å½’+å°æ­¥é¢„æµ‹ï¼‰ï¼Œä¿ç•™æ–°ä»£ç çš„æ¥å£è§„èŒƒ
    :param dataB: è¾“å…¥æ•°æ®ï¼ˆDataFrameï¼Œå«Voltageåˆ—ï¼‰
    :param seq_length: çª—å£å¤§å°ï¼ˆé»˜è®¤32ï¼Œå¯¹é½æ—§ä»£ç ï¼‰
    :param save_path: ä¿å­˜è·¯å¾„ï¼ˆé»˜è®¤å½“å‰ç›®å½•ï¼‰
    :param ratio: æ­¥é•¿æ¯”ä¾‹ï¼ˆé»˜è®¤2 â†’ future_steps=16ï¼Œå¯¹é½æ—§ä»£ç ï¼‰
    :return: é¢„æµ‹ç‚¹æ—¶é—´ç´¢å¼•ã€é€†æ ‡å‡†åŒ–åçš„é¢„æµ‹å€¼
    """
    # 1. åŠ è½½Scalerå’Œæ¨¡å‹ï¼ˆæ–°ä»£ç çš„æ¥å£é€»è¾‘ï¼‰
    scaler_path = os.path.join(save_path, 'scaler_piezo.pkl')
    model_weights_path = os.path.join(save_path, 'lstm_model_weights.h5')
    with open(scaler_path, 'rb') as f:
        scaler = pickle.load(f)
    model = build_unified_model(seq_length)
    model.load_weights(model_weights_path)
    print(f"âœ… æˆåŠŸåŠ è½½æ¨¡å‹æƒé‡ï¼š{model_weights_path}")

    # 2. æ•°æ®æ ‡å‡†åŒ–ï¼ˆå¯¹é½æ—§ä»£ç ï¼‰
    data_scaled = scaler.transform(dataB[['Voltage']])
    X, y = create_sequences(data_scaled, seq_length)

    # 3. æ—§ä»£ç çš„80-20åˆ’åˆ†ï¼šåªé¢„æµ‹æµ‹è¯•é›†
    split_index = int(len(X) * 0.8)
    X_test = X[split_index:]
    y_test = y[split_index:]

    # 4. æ—§ä»£ç çš„æ ¸å¿ƒï¼šå°æ­¥é¢„æµ‹ï¼ˆfuture_steps=seq_length//ratioï¼‰
    future_steps = seq_length // ratio
    total_steps = len(y_test) // future_steps
    all_pred_data = []

    # 5. æ—§ä»£ç çš„æ ¸å¿ƒï¼šè‡ªå›å½’é¢„æµ‹ï¼ˆé¢„æµ‹å€¼å¡å›çª—å£ï¼‰
    for step in range(total_steps):
        start_index = step * future_steps
        if start_index >= len(X_test):
            break

        # å–æµ‹è¯•é›†çœŸå®çª—å£
        last_sequence = X_test[start_index].reshape(1, seq_length, 1)
        predicted_data = []

        # è‡ªå›å½’ï¼šé¢„æµ‹ä¸€ä¸ªç‚¹ï¼Œå¡å›çª—å£ï¼Œå†é¢„æµ‹ä¸‹ä¸€ä¸ª
        for i in range(future_steps):
            pred = model.predict(last_sequence, verbose=0)[0, 0]
            predicted_data.append(pred)
            # å…³é”®ï¼šnp.rollæ»šåŠ¨çª—å£ï¼ŒæŠŠé¢„æµ‹å€¼å¡å›ï¼ˆæ—§ä»£ç æ ¸å¿ƒï¼‰
            last_sequence = np.roll(last_sequence, -1, axis=1)
            last_sequence[0, -1, 0] = pred

        all_pred_data.extend(predicted_data)

    # 6. é€†æ ‡å‡†åŒ–ï¼ˆæ–°ä»£ç çš„å¤„ç†é€»è¾‘ï¼‰
    all_pred_data = np.array(all_pred_data).reshape(-1, 1)
    pred_data_inversed = scaler.inverse_transform(all_pred_data)

    # 7. é¢„æµ‹ç‚¹æ—¶é—´ç´¢å¼•ï¼ˆå¯¹é½æ–°ä»£ç çš„æ¥å£ï¼‰
    pred_time = np.arange(split_index + seq_length, split_index + seq_length + len(pred_data_inversed))

    # 8. ä¿ç•™æ–°ä»£ç çš„è¾“å‡ºï¼ˆä»…æ ¸å¿ƒè¾“å‡ºï¼Œä¸åŠ é¢å¤–åŠŸèƒ½ï¼‰
    print(f"\nâœ… predict_oldé¢„æµ‹å®Œæˆï¼š")
    print(f"   - çª—å£å¤§å°ï¼š{seq_length} | å•æ¬¡é¢„æµ‹æ­¥é•¿ï¼š{future_steps}")
    print(f"   - é¢„æµ‹å€¼èŒƒå›´ï¼š{pred_data_inversed.min():.4f} ~ {pred_data_inversed.max():.4f}")
    print(f"   - æ€»é¢„æµ‹ç‚¹æ•°ï¼š{len(pred_data_inversed)}")

    return pred_time, pred_data_inversed


# ========== é€‚é…å±€éƒ¨æ ‡å‡†åŒ–çš„predict_oldå‡½æ•° ==========
def predict_old_local_scaler(dataB, seq_length=32, save_path='./', ratio=2):
    """
    é€‚é…å±€éƒ¨çª—å£æ ‡å‡†åŒ–çš„é¢„æµ‹å‡½æ•°ï¼ˆè‡ªå›å½’+å°æ­¥é¢„æµ‹ï¼‰
    :param dataB: è¾“å…¥æ•°æ®ï¼ˆDataFrameï¼Œå«Voltageåˆ—ï¼‰
    :param seq_length: çª—å£å¤§å°
    :param save_path: æ¨¡å‹/scalerä¿å­˜è·¯å¾„
    :param ratio: æ­¥é•¿æ¯”ä¾‹ï¼ˆé»˜è®¤2 â†’ future_steps=16ï¼‰
    :return: é¢„æµ‹ç‚¹æ—¶é—´ç´¢å¼•ã€é€†æ ‡å‡†åŒ–åçš„é¢„æµ‹å€¼
    """
    # 1. åŠ è½½æ¨¡å‹å’Œscalerï¼ˆé€‚é…å±€éƒ¨æ ‡å‡†åŒ–çš„è®­ç»ƒå‡½æ•°è¾“å‡ºï¼‰
    model_weights_path = os.path.join(save_path, 'lstm_model_weights_local.h5')
    window_scalers_path = os.path.join(save_path, 'window_scalers.pkl')
    global_scaler_path = os.path.join(save_path, 'global_scaler_piezo.pkl')

    # åŠ è½½æ¨¡å‹
    model = build_unified_model(seq_length)
    model.load_weights(model_weights_path)
    print(f"âœ… æˆåŠŸåŠ è½½æ¨¡å‹æƒé‡ï¼š{model_weights_path}")

    # åŠ è½½çª—å£scalerå’Œå…¨å±€å‚è€ƒscaler
    with open(window_scalers_path, 'rb') as f:
        window_scalers = pickle.load(f)
    with open(global_scaler_path, 'rb') as f:
        global_scaler = pickle.load(f)
    print(f"âœ… åŠ è½½{len(window_scalers)}ä¸ªçª—å£çš„å±€éƒ¨scaler")

    # 2. 80-20åˆ’åˆ†ï¼ˆç”¨åŸå§‹æ•°æ®ï¼Œä¸åšå…¨å±€æ ‡å‡†åŒ–ï¼‰
    raw_data = dataB[['Voltage']].values
    total_samples_raw = len(raw_data) - seq_length
    split_index = int(total_samples_raw * 0.8)
    X_test_raw = raw_data[split_index: split_index + total_samples_raw - split_index + seq_length]

    # 3. æ—§ä»£ç æ ¸å¿ƒï¼šå°æ­¥é¢„æµ‹å‚æ•°
    future_steps = seq_length // ratio
    total_steps = (total_samples_raw - split_index) // future_steps
    all_pred_data = []
    pred_time_list = []

    # 4. è‡ªå›å½’é¢„æµ‹ï¼ˆæ ¸å¿ƒé€»è¾‘ï¼Œé€‚é…å±€éƒ¨scalerï¼‰
    for step in range(total_steps):
        start_index = split_index + step * future_steps
        if start_index + seq_length > len(raw_data):
            break

        # æ‰¾åˆ°å½“å‰é¢„æµ‹çª—å£æ‰€å±çš„å±€éƒ¨scaler
        # åŒ¹é…è§„åˆ™ï¼šæ‰¾åˆ°æœ€æ¥è¿‘å½“å‰start_indexçš„çª—å£scaler
        window_starts = sorted(window_scalers.keys())
        target_window_start = None
        for ws in window_starts:
            if ws <= start_index < ws + int(total_samples_raw * 0.1):  # 0.1æ˜¯è®­ç»ƒæ—¶çš„window_ratio
                target_window_start = ws
                break
        # å¦‚æœæ²¡æ‰¾åˆ°åŒ¹é…çš„çª—å£scalerï¼Œç”¨å…¨å±€scalerå…œåº•
        if target_window_start is None:
            target_scaler = global_scaler
            print(f"âš ï¸ é¢„æµ‹çª—å£{start_index}æœªåŒ¹é…åˆ°å±€éƒ¨scalerï¼Œä½¿ç”¨å…¨å±€scaler")
        else:
            target_scaler = window_scalers[target_window_start]
            print(f"âœ… é¢„æµ‹çª—å£{start_index}åŒ¹é…åˆ°å±€éƒ¨scalerï¼ˆçª—å£èµ·å§‹ï¼š{target_window_start}ï¼‰")

        # å–å½“å‰çª—å£çš„åŸå§‹æ•°æ®ï¼Œåšå±€éƒ¨æ ‡å‡†åŒ–
        current_raw_window = raw_data[start_index: start_index + seq_length]
        current_scaled_window = target_scaler.transform(current_raw_window)
        last_sequence = current_scaled_window.reshape(1, seq_length, 1)
        predicted_data_scaled = []

        # è‡ªå›å½’é¢„æµ‹ï¼ˆå°æ­¥ï¼‰
        for i in range(future_steps):
            pred_scaled = model.predict(last_sequence, verbose=0)[0, 0]
            predicted_data_scaled.append(pred_scaled)
            # æ»šåŠ¨çª—å£ï¼šé¢„æµ‹å€¼å¡å›ï¼ˆç”¨æ ‡å‡†åŒ–åçš„å€¼ï¼‰
            last_sequence = np.roll(last_sequence, -1, axis=1)
            last_sequence[0, -1, 0] = pred_scaled

        # é€†æ ‡å‡†åŒ–ï¼šç”¨åŒ¹é…çš„å±€éƒ¨scalerè¿˜åŸå¹…å€¼
        predicted_data_scaled = np.array(predicted_data_scaled).reshape(-1, 1)
        predicted_data_inversed = target_scaler.inverse_transform(predicted_data_scaled)
        all_pred_data.extend(predicted_data_inversed.flatten().tolist())

        # è®°å½•é¢„æµ‹æ—¶é—´ç´¢å¼•
        pred_start = start_index + seq_length
        pred_time_list.extend(range(pred_start, pred_start + future_steps))

    # 5. æ•´ç†è¾“å‡º
    all_pred_data = np.array(all_pred_data).reshape(-1, 1)
    pred_time = np.array(pred_time_list[:len(all_pred_data)])  # å¯¹é½é•¿åº¦

    print(f"\nâœ… predict_oldï¼ˆå±€éƒ¨æ ‡å‡†åŒ–ï¼‰é¢„æµ‹å®Œæˆï¼š")
    print(f"   - çª—å£å¤§å°ï¼š{seq_length} | å•æ¬¡é¢„æµ‹æ­¥é•¿ï¼š{future_steps}")
    print(f"   - é¢„æµ‹å€¼èŒƒå›´ï¼š{all_pred_data.min():.4f} ~ {all_pred_data.max():.4f}")
    print(f"   - æ€»é¢„æµ‹ç‚¹æ•°ï¼š{len(all_pred_data)}")

    return pred_time, all_pred_data



def plot_predicted_data(time_data, predicted_data_inversed):
    # ç»˜åˆ¶é¢„æµ‹ç»“æœ
    plt.plot(time_data, predicted_data_inversed, label="Predicted Voltage")
    plt.xlabel("Time")
    plt.ylabel("Voltage")
    plt.legend()
    plt.show()


def plot_double_figure(true_data, time_true, pred_data, time_pred):
    """ç»˜åˆ¶çœŸå®/é¢„æµ‹å¯¹æ¯”å›¾"""
    # å›¾1ï¼šçœŸå® vs é¢„æµ‹
    plt.figure(figsize=(12, 5))
    plt.plot(time_true, true_data['Voltage'].values, label='çœŸå®ç”µå‹', alpha=0.7, color='blue')
    plt.plot(time_pred, pred_data, label='é¢„æµ‹ç”µå‹', alpha=0.7, color='red')
    plt.xlabel('é‡‡æ ·ç‚¹åºå·')
    plt.ylabel('ç”µå‹ (V)')
    plt.title('çœŸå®ç”µå‹ vs é¢„æµ‹ç”µå‹å¯¹æ¯”ï¼ˆé‡‡æ ·ç‚¹åºå·ï¼‰')
    plt.legend()
    plt.grid(alpha=0.3)
    plt.tight_layout()
    plt.show()

    # å›¾2ï¼šä»…é¢„æµ‹ç”µå‹
    plt.figure(figsize=(12, 5))
    plt.plot(time_pred, pred_data, color='red', label='é¢„æµ‹ç”µå‹')
    plt.xlabel('é‡‡æ ·ç‚¹åºå·')
    plt.ylabel('ç”µå‹ (V)')
    plt.title('é¢„æµ‹ç”µå‹éšé‡‡æ ·ç‚¹å˜åŒ–')
    plt.legend()
    plt.grid(alpha=0.3)
    plt.tight_layout()
    plt.show()